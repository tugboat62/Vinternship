{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport spacy\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\ndata = pd.read_csv('/kaggle/input/bankOffers/Transaction_Naration_Data_Set.csv')  # Replace 'transactions.csv' with the actual filename\n\n# Perform any necessary preprocessing steps, such as removing noise or irrelevant characters\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download en_core_web_md ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import words\n\n# Load the English word corpus\nnltk.download('words')\nenglish_words = set(words.words())\n\n# Function to check if a word is an English word\ndef is_english_word(word):\n    return word.lower() in english_words\n\n# Filter rows that contain English words in the 'Naration' column\ndata = data[data['NARATION'].apply(lambda x: isinstance(x, str) and any(is_english_word(word) for word in x.split()))]","metadata":{"execution":{"iopub.status.busy":"2023-05-29T18:27:58.713921Z","iopub.execute_input":"2023-05-29T18:27:58.714429Z","iopub.status.idle":"2023-05-29T18:28:54.123972Z","shell.execute_reply.started":"2023-05-29T18:27:58.714394Z","shell.execute_reply":"2023-05-29T18:28:54.122568Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package words to /usr/share/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"data['NARATION'] = data['NARATION'].str.replace('[^A-Za-z0-9]+', ' ', regex=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T18:29:09.280396Z","iopub.execute_input":"2023-05-29T18:29:09.280900Z","iopub.status.idle":"2023-05-29T18:30:42.805924Z","shell.execute_reply.started":"2023-05-29T18:29:09.280863Z","shell.execute_reply":"2023-05-29T18:30:42.804791Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3125027333.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['NARATION'] = data['NARATION'].str.replace('[^A-Za-z0-9]+', ' ', regex=True)\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0    AC_ID    AMOUNT   DOC_DATE  \\\n0           0  3644877  342800.0  01-NOV-22   \n1           1  1175256  342800.0  01-NOV-22   \n2           2  2298226    1500.0  01-NOV-22   \n3           3  4066427    1500.0  01-NOV-22   \n4           4  1978564    2550.0  01-NOV-22   \n\n                                            NARATION   DR_CR  \n0                                      Cash Withdraw   Debit  \n1                       Cash Withdrawal From A C No   Credit  \n2    Cash withdraw from Micro Merchant point MM A C    Debit  \n3  Credit Against Merchant Cash Withdraw Service ...  Credit  \n4    Cash withdraw from Micro Merchant point MM A C    Debit  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>AC_ID</th>\n      <th>AMOUNT</th>\n      <th>DOC_DATE</th>\n      <th>NARATION</th>\n      <th>DR_CR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3644877</td>\n      <td>342800.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash Withdraw</td>\n      <td>Debit</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1175256</td>\n      <td>342800.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash Withdrawal From A C No</td>\n      <td>Credit</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2298226</td>\n      <td>1500.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash withdraw from Micro Merchant point MM A C</td>\n      <td>Debit</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4066427</td>\n      <td>1500.0</td>\n      <td>01-NOV-22</td>\n      <td>Credit Against Merchant Cash Withdraw Service ...</td>\n      <td>Credit</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1978564</td>\n      <td>2550.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash withdraw from Micro Merchant point MM A C</td>\n      <td>Debit</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the pre-trained SpaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef eraseName(text):\n    doc = nlp(text)\n    # Remove named entities of type \"PERSON\" and \"GPE\"\n    cleaned_text = []\n    for token in doc:\n        if token.ent_type_ not in [\"PERSON\", \"GPE\"]:\n            cleaned_text.append(token.text)\n\n    # Join the cleaned tokens back into a string\n    cleaned_text = \" \".join(cleaned_text)\n    return cleaned_text\n\ndata['NARATION'] = data['NARATION'].apply(eraseName)\nprint(data.head)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:43:36.352170Z","iopub.execute_input":"2023-05-29T21:43:36.352694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:42:42.818851Z","iopub.execute_input":"2023-05-29T21:42:42.819352Z","iopub.status.idle":"2023-05-29T21:42:42.838466Z","shell.execute_reply.started":"2023-05-29T21:42:42.819309Z","shell.execute_reply":"2023-05-29T21:42:42.836977Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0    AC_ID    AMOUNT   DOC_DATE  \\\n0           0  3644877  342800.0  01-NOV-22   \n1           1  1175256  342800.0  01-NOV-22   \n2           2  2298226    1500.0  01-NOV-22   \n3           3  4066427    1500.0  01-NOV-22   \n4           4  1978564    2550.0  01-NOV-22   \n\n                                            NARATION   DR_CR  \n0                                      Cash Withdraw   Debit  \n1                       Cash Withdrawal From A C No   Credit  \n2    Cash withdraw from Micro Merchant point MM A C    Debit  \n3  Credit Against Merchant Cash Withdraw Service ...  Credit  \n4    Cash withdraw from Micro Merchant point MM A C    Debit  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>AC_ID</th>\n      <th>AMOUNT</th>\n      <th>DOC_DATE</th>\n      <th>NARATION</th>\n      <th>DR_CR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3644877</td>\n      <td>342800.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash Withdraw</td>\n      <td>Debit</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1175256</td>\n      <td>342800.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash Withdrawal From A C No</td>\n      <td>Credit</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2298226</td>\n      <td>1500.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash withdraw from Micro Merchant point MM A C</td>\n      <td>Debit</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4066427</td>\n      <td>1500.0</td>\n      <td>01-NOV-22</td>\n      <td>Credit Against Merchant Cash Withdraw Service ...</td>\n      <td>Credit</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1978564</td>\n      <td>2550.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash withdraw from Micro Merchant point MM A C</td>\n      <td>Debit</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['NARATION'] = data['NARATION'].str.lower()\ndata['DR_CR'] = data['DR_CR'].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n# Perform tokenization, stemming, and stop word removal\nstop_words = set(stopwords.words('english'))\nstop_words |= {'cash', 'a', 'c', 'no'}\n\ndata['NARATION'].fillna('', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Define a function to perform lemmatization on a text\ndef lemmatize_text(text):\n    # Tokenize the text into words\n    tokens = word_tokenize(text)\n    # Lemmatize each word and join them back into a sentence\n    lemmatized_text = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    return lemmatized_text\n\n# Apply lemmatization to the \"NARATION\" column\ndata['Lem_Tokens'] = data['NARATION'].apply(lemmatize_text)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Assuming you have a list of tokenized sentences named 'tokens_list'\n\n# Convert the tokenized sentences into strings\nsentences = [' '.join(tokens) for tokens in data['Tokens']]\n\n# Initialize the TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the sentences to obtain the TF-IDF matrix\ntfidf_matrix = vectorizer.fit_transform(sentences)\n\n# Get the feature words (vocabulary) from the vectorizer\nfeature_words = vectorizer.vocabulary_\n\n# Print the feature words\nprint(feature_words)\nprint(data.head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_words_per_row = [list(set(row_tokens) & set(feature_words)) for row_tokens in data['Tokens']]\n\n# data = data.assign(FeatureWords=[list(set(row_tokens) & set(feature_words)) for row_tokens in data['Tokens']])\n# print(data.head())\n\nfrom sklearn.cluster import KMeans\n\n# Define the number of clusters\nnum_clusters = 5\namounts_array = data['AMOUNT'].to_numpy()\n\n# Create a feature matrix by combining feature words and amounts\ndata_matrix = np.column_stack((feature_words_per_row, amounts_array))\n\n# Perform K-means clustering\nkmeans = KMeans(n_clusters=num_clusters)\nkmeans.fit(data_matrix)\n\n# Get the cluster labels\ncluster_labels = kmeans.labels_\n\n# Assign the cluster labels to your original dataframe\ndata['Cluster'] = cluster_labels\nprint(data.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.cluster import KMeans\n\n# # Define the number of clusters\n# num_clusters = 5\n# amounts_array = data['AMOUNT'].to_numpy()\n\n# # Create a feature matrix by combining feature words and amounts\n# data_matrix = np.column_stack((feature_words_per_row, amounts_array))\n\n# # Perform K-means clustering\n# kmeans = KMeans(n_clusters=num_clusters)\n# kmeans.fit(data_matrix)\n\n# # Get the cluster labels\n# cluster_labels = kmeans.labels_\n\n# # Assign the cluster labels to your original dataframe\n# data['Cluster'] = cluster_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.cluster import MiniBatchKMeans\n# from sklearn.feature_extraction.text import CountVectorizer\n\n\n# vectorizer = CountVectorizer()\n\n# # Fit and transform the documents to obtain the document-term matrix\n# X = vectorizer.fit_transform(data['NARATION'])\n\n# # Initialize the MiniBatchKMeans algorithm\n# kmeans = MiniBatchKMeans(n_clusters=3, random_state=42)\n\n# # Fit the algorithm on the document-term matrix\n# kmeans.fit(X)\n\n# # Get the cluster labels for each document\n# labels = kmeans.labels_\n \n# vec = TfidfVectorizer(stop_words=\"english\")\n# vec.fit(data.NARATION.values)\n# keywords = vec.transform(data.NARATION.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_data2 = data.groupby('AC_ID')\n\n# # Create a new DataFrame with a list of tuples for each distinct 'ac_no'\n# grouped_df = pd.DataFrame([(AC_ID, list(zip(group['AMOUNT'], group['DR_CR'], group['TOKENS'])))\n#                            for AC_ID, group in grouped_data2],\n#                           columns=['AC_ID', 'TRANSACTIONS'])\n# grouped_df.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# # Assuming 'grouped_df' is the DataFrame containing the 'transactions' column with a list of tuples\n# # Assuming the column name is 'transactions'\n# # def flat(d) -> str:\n# #     l = []\n# #     for tuple_item in d:\n# #         s = \"\"\n# #         for i in tuple_item:\n# #             s = s + \" \" + str(i)\n# #         l.append(s)\n# #     return l\n                        \n\n# # Flatten the 'transactions' column to create a list of strings\n# grouped_df['TRANSACTIONS'] = [[str(' '.join(str(element) for element in tuple_element)) for tuple_element in row] for row in grouped_df['TRANSACTIONS']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_df.head()\n# Initialize the TfidfVectorizer\n# vectorizer = TfidfVectorizer()\n\n# # Compute the TF-IDF features\n# tfidf_features = vectorizer.fit_transform(flatten_transactions)\n# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# # Assuming you have a dataframe named 'df' with a column 'text' containing the list of strings\n# # For example:\n# df = pd.DataFrame({'text': [['apple', 'banana', 'orange'], ['grape', 'kiwi']]})\n# print(df)\n\n# # Flatten the list of strings within each row\n# flattened_text = [word for sublist in df['text'] for word in sublist]\n# print(flattened_text)\n\n# # Initialize the TfidfVectorizer\n# vectorizer = TfidfVectorizer()\n\n# # Fit and transform the flattened text data to obtain the TF-IDF matrix\n# tfidf_matrix = vectorizer.fit_transform(flattened_text)\n\n\n# # Print the feature words\n# print(tfidf_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Convert the TF-IDF features to a DataFrame\n# tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=vectorizer.get_feature_names())\n\n# # Print the TF-IDF DataFrame\n# tfidf_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}