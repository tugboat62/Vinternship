{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport spacy\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk import ne_chunk, pos_tag\nfrom nltk.tree import Tree\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\ndata = pd.read_csv('/kaggle/input/bankOffers/Transaction_Naration_Data_Set.csv')  # Replace 'transactions.csv' with the actual filename\n\n# Perform any necessary preprocessing steps, such as removing noise or irrelevant characters\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-31T05:14:06.938788Z","iopub.execute_input":"2023-05-31T05:14:06.939662Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/bankOffers/Transaction_Naration_Data_Set.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-30T21:46:45.454937Z","iopub.execute_input":"2023-05-30T21:46:45.455291Z","iopub.status.idle":"2023-05-30T21:46:48.594544Z","shell.execute_reply.started":"2023-05-30T21:46:45.455261Z","shell.execute_reply":"2023-05-30T21:46:48.593354Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"         Unnamed: 0         AC_ID        AMOUNT\ncount  2.254676e+07  2.254676e+07  2.254676e+07\nmean   1.127338e+07  2.765147e+06  2.075528e+04\nstd    6.508689e+06  1.984404e+06  8.234750e+04\nmin    0.000000e+00  1.000000e+00  1.000000e-02\n25%    5.636689e+06  6.461500e+05  6.000000e+02\n50%    1.127338e+07  2.942007e+06  1.500000e+03\n75%    1.691007e+07  4.529672e+06  7.000000e+03\nmax    2.254676e+07  6.395404e+06  4.150000e+07","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>AC_ID</th>\n      <th>AMOUNT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.254676e+07</td>\n      <td>2.254676e+07</td>\n      <td>2.254676e+07</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.127338e+07</td>\n      <td>2.765147e+06</td>\n      <td>2.075528e+04</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>6.508689e+06</td>\n      <td>1.984404e+06</td>\n      <td>8.234750e+04</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e-02</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.636689e+06</td>\n      <td>6.461500e+05</td>\n      <td>6.000000e+02</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.127338e+07</td>\n      <td>2.942007e+06</td>\n      <td>1.500000e+03</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.691007e+07</td>\n      <td>4.529672e+06</td>\n      <td>7.000000e+03</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.254676e+07</td>\n      <td>6.395404e+06</td>\n      <td>4.150000e+07</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def contains_english_words(text):\n    if isinstance(text, str):\n        english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n        return bool(english_words)\n    return False\n\n# Filter the DataFrame to keep only rows with English words\ndata = data[data['NARATION'].apply(contains_english_words)]","metadata":{"execution":{"iopub.status.busy":"2023-05-30T21:46:48.595904Z","iopub.execute_input":"2023-05-30T21:46:48.596250Z","iopub.status.idle":"2023-05-30T21:48:45.112299Z","shell.execute_reply.started":"2023-05-30T21:46:48.596221Z","shell.execute_reply":"2023-05-30T21:48:45.111397Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"data.describe()\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-30T21:48:45.114729Z","iopub.execute_input":"2023-05-30T21:48:45.115238Z","iopub.status.idle":"2023-05-30T21:48:48.143424Z","shell.execute_reply.started":"2023-05-30T21:48:45.115209Z","shell.execute_reply":"2023-05-30T21:48:48.142345Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"         Unnamed: 0         AC_ID        AMOUNT\ncount  2.254400e+07  2.254400e+07  2.254400e+07\nmean   1.127351e+07  2.765063e+06  2.075346e+04\nstd    6.508715e+06  1.984371e+06  8.234887e+04\nmin    0.000000e+00  1.000000e+00  1.000000e-02\n25%    5.636795e+06  6.461500e+05  6.000000e+02\n50%    1.127368e+07  2.941875e+06  1.500000e+03\n75%    1.691018e+07  4.529190e+06  7.000000e+03\nmax    2.254676e+07  6.395404e+06  4.150000e+07","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>AC_ID</th>\n      <th>AMOUNT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.254400e+07</td>\n      <td>2.254400e+07</td>\n      <td>2.254400e+07</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.127351e+07</td>\n      <td>2.765063e+06</td>\n      <td>2.075346e+04</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>6.508715e+06</td>\n      <td>1.984371e+06</td>\n      <td>8.234887e+04</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e-02</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.636795e+06</td>\n      <td>6.461500e+05</td>\n      <td>6.000000e+02</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.127368e+07</td>\n      <td>2.941875e+06</td>\n      <td>1.500000e+03</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.691018e+07</td>\n      <td>4.529190e+06</td>\n      <td>7.000000e+03</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.254676e+07</td>\n      <td>6.395404e+06</td>\n      <td>4.150000e+07</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!python -m spacy download en_core_web_md ","metadata":{"execution":{"iopub.status.busy":"2023-05-30T21:48:48.144703Z","iopub.execute_input":"2023-05-30T21:48:48.145984Z","iopub.status.idle":"2023-05-30T21:49:22.747031Z","shell.execute_reply.started":"2023-05-30T21:48:48.145947Z","shell.execute_reply":"2023-05-30T21:49:22.744850Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nCollecting en-core-web-md==3.5.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-md==3.5.0) (3.5.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\nRequirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.64.1)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.2)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.23.5)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (59.8.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (21.3)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_md')\n","output_type":"stream"}]},{"cell_type":"code","source":"data['NARATION'].fillna('', inplace=True)\ndata['NARATION'] = data['NARATION'].str.replace('[^A-Za-z0-9]+', ' ', regex=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-30T21:49:22.750852Z","iopub.execute_input":"2023-05-30T21:49:22.751484Z","iopub.status.idle":"2023-05-30T21:51:06.912860Z","shell.execute_reply.started":"2023-05-30T21:49:22.751425Z","shell.execute_reply":"2023-05-30T21:51:06.911743Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0    AC_ID    AMOUNT   DOC_DATE  \\\n0           0  3644877  342800.0  01-NOV-22   \n1           1  1175256  342800.0  01-NOV-22   \n2           2  2298226    1500.0  01-NOV-22   \n3           3  4066427    1500.0  01-NOV-22   \n4           4  1978564    2550.0  01-NOV-22   \n\n                                            NARATION   DR_CR  \n0                                      Cash Withdraw   Debit  \n1                       Cash Withdrawal From A C No   Credit  \n2    Cash withdraw from Micro Merchant point MM A C    Debit  \n3  Credit Against Merchant Cash Withdraw Service ...  Credit  \n4    Cash withdraw from Micro Merchant point MM A C    Debit  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>AC_ID</th>\n      <th>AMOUNT</th>\n      <th>DOC_DATE</th>\n      <th>NARATION</th>\n      <th>DR_CR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3644877</td>\n      <td>342800.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash Withdraw</td>\n      <td>Debit</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1175256</td>\n      <td>342800.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash Withdrawal From A C No</td>\n      <td>Credit</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2298226</td>\n      <td>1500.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash withdraw from Micro Merchant point MM A C</td>\n      <td>Debit</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4066427</td>\n      <td>1500.0</td>\n      <td>01-NOV-22</td>\n      <td>Credit Against Merchant Cash Withdraw Service ...</td>\n      <td>Credit</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1978564</td>\n      <td>2550.0</td>\n      <td>01-NOV-22</td>\n      <td>Cash withdraw from Micro Merchant point MM A C</td>\n      <td>Debit</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Function to remove names from a text using NLTK\ndef remove_names(text):\n    tokens = word_tokenize(text)\n    nltk_results = ne_chunk(pos_tag(tokens))\n    for nltk_result in nltk_results:\n        if type(nltk_result) == Tree:\n            for nltk_result_leaf in nltk_result.leaves():\n                tokens.remove(nltk_result_leaf[0])\n    filtered_text = ' '.join(tokens)\n    return filtered_text\n\n# Apply the name removal function to the \"NARATION\" column\ndata['NARATION'] = data['NARATION'].apply(remove_names)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-30T21:51:06.914818Z","iopub.execute_input":"2023-05-30T21:51:06.915153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['NARATION'] = data['NARATION'].str.lower()\ndata['DR_CR'] = data['DR_CR'].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n# Perform tokenization, stemming, and stop word removal\nstop_words = set(stopwords.words('english'))\nstop_words |= {'cash', 'a', 'c', 'no'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initialize the lemmatizer\n# lemmatizer = WordNetLemmatizer()\n\n# # Define a function to perform lemmatization on a text\n# def lemmatize_text(text):\n#     # Tokenize the text into words\n#     tokens = word_tokenize(text)\n#     # Lemmatize each word and join them back into a sentence\n#     lemmatized_text = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n#     return lemmatized_text\n\n# # Apply lemmatization to the \"NARATION\" column\n# data['Lem_Tokens'] = data['NARATION'].apply(lemmatize_text)\n# data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# # Assuming you have a list of tokenized sentences named 'tokens_list'\n\n# # Convert the tokenized sentences into strings\n# sentences = [' '.join(tokens) for tokens in data['Tokens']]\n\n# # Initialize the TfidfVectorizer\n# vectorizer = TfidfVectorizer()\n\n# # Fit and transform the sentences to obtain the TF-IDF matrix\n# tfidf_matrix = vectorizer.fit_transform(sentences)\n\n# # Get the feature words (vocabulary) from the vectorizer\n# feature_words = vectorizer.vocabulary_\n\n# # Print the feature words\n# print(feature_words)\n# print(data.head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature_words_per_row = [list(set(row_tokens) & set(feature_words)) for row_tokens in data['Tokens']]\n\n# # data = data.assign(FeatureWords=[list(set(row_tokens) & set(feature_words)) for row_tokens in data['Tokens']])\n# # print(data.head())\n\n# from sklearn.cluster import KMeans\n\n# # Define the number of clusters\n# num_clusters = 5\n# amounts_array = data['AMOUNT'].to_numpy()\n\n# # Create a feature matrix by combining feature words and amounts\n# data_matrix = np.column_stack((feature_words_per_row, amounts_array))\n\n# # Perform K-means clustering\n# kmeans = KMeans(n_clusters=num_clusters)\n# kmeans.fit(data_matrix)\n\n# # Get the cluster labels\n# cluster_labels = kmeans.labels_\n\n# # Assign the cluster labels to your original dataframe\n# data['Cluster'] = cluster_labels\n# print(data.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.cluster import KMeans\n\n# # Define the number of clusters\n# num_clusters = 5\n# amounts_array = data['AMOUNT'].to_numpy()\n\n# # Create a feature matrix by combining feature words and amounts\n# data_matrix = np.column_stack((feature_words_per_row, amounts_array))\n\n# # Perform K-means clustering\n# kmeans = KMeans(n_clusters=num_clusters)\n# kmeans.fit(data_matrix)\n\n# # Get the cluster labels\n# cluster_labels = kmeans.labels_\n\n# # Assign the cluster labels to your original dataframe\n# data['Cluster'] = cluster_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.cluster import MiniBatchKMeans\n# from sklearn.feature_extraction.text import CountVectorizer\n\n\n# vectorizer = CountVectorizer()\n\n# # Fit and transform the documents to obtain the document-term matrix\n# X = vectorizer.fit_transform(data['NARATION'])\n\n# # Initialize the MiniBatchKMeans algorithm\n# kmeans = MiniBatchKMeans(n_clusters=3, random_state=42)\n\n# # Fit the algorithm on the document-term matrix\n# kmeans.fit(X)\n\n# # Get the cluster labels for each document\n# labels = kmeans.labels_\n \n# vec = TfidfVectorizer(stop_words=\"english\")\n# vec.fit(data.NARATION.values)\n# keywords = vec.transform(data.NARATION.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_data2 = data.groupby('AC_ID')\n\n# # Create a new DataFrame with a list of tuples for each distinct 'ac_no'\n# grouped_df = pd.DataFrame([(AC_ID, list(zip(group['AMOUNT'], group['DR_CR'], group['TOKENS'])))\n#                            for AC_ID, group in grouped_data2],\n#                           columns=['AC_ID', 'TRANSACTIONS'])\n# grouped_df.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# # Assuming 'grouped_df' is the DataFrame containing the 'transactions' column with a list of tuples\n# # Assuming the column name is 'transactions'\n# # def flat(d) -> str:\n# #     l = []\n# #     for tuple_item in d:\n# #         s = \"\"\n# #         for i in tuple_item:\n# #             s = s + \" \" + str(i)\n# #         l.append(s)\n# #     return l\n                        \n\n# # Flatten the 'transactions' column to create a list of strings\n# grouped_df['TRANSACTIONS'] = [[str(' '.join(str(element) for element in tuple_element)) for tuple_element in row] for row in grouped_df['TRANSACTIONS']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_df.head()\n# Initialize the TfidfVectorizer\n# vectorizer = TfidfVectorizer()\n\n# # Compute the TF-IDF features\n# tfidf_features = vectorizer.fit_transform(flatten_transactions)\n# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# # Assuming you have a dataframe named 'df' with a column 'text' containing the list of strings\n# # For example:\n# df = pd.DataFrame({'text': [['apple', 'banana', 'orange'], ['grape', 'kiwi']]})\n# print(df)\n\n# # Flatten the list of strings within each row\n# flattened_text = [word for sublist in df['text'] for word in sublist]\n# print(flattened_text)\n\n# # Initialize the TfidfVectorizer\n# vectorizer = TfidfVectorizer()\n\n# # Fit and transform the flattened text data to obtain the TF-IDF matrix\n# tfidf_matrix = vectorizer.fit_transform(flattened_text)\n\n\n# # Print the feature words\n# print(tfidf_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Convert the TF-IDF features to a DataFrame\n# tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=vectorizer.get_feature_names())\n\n# # Print the TF-IDF DataFrame\n# tfidf_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}