{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport spacy\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk import ne_chunk, pos_tag\nfrom nltk.tree import Tree\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\ndata = pd.read_csv('/kaggle/input/bankOffers/Transaction_Naration_Data_Set.csv')  # Replace 'transactions.csv' with the actual filename\n\n# Perform any necessary preprocessing steps, such as removing noise or irrelevant characters\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-04T18:26:07.339411Z","iopub.execute_input":"2023-06-04T18:26:07.340058Z","iopub.status.idle":"2023-06-04T18:26:32.997240Z","shell.execute_reply.started":"2023-06-04T18:26:07.340025Z","shell.execute_reply":"2023-06-04T18:26:32.995931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:26:33.001441Z","iopub.execute_input":"2023-06-04T18:26:33.001825Z","iopub.status.idle":"2023-06-04T18:26:35.343334Z","shell.execute_reply.started":"2023-06-04T18:26:33.001781Z","shell.execute_reply":"2023-06-04T18:26:35.342266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def contains_english_words(text):\n    if isinstance(text, str):\n        english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n        return bool(english_words)\n    return False\n\n# Filter the DataFrame to keep only rows with English words\ndata = data[data['NARATION'].apply(contains_english_words)]","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:26:35.345008Z","iopub.execute_input":"2023-06-04T18:26:35.345694Z","iopub.status.idle":"2023-06-04T18:28:25.723222Z","shell.execute_reply.started":"2023-06-04T18:26:35.345655Z","shell.execute_reply":"2023-06-04T18:28:25.722001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:28:25.726456Z","iopub.execute_input":"2023-06-04T18:28:25.726775Z","iopub.status.idle":"2023-06-04T18:28:28.002056Z","shell.execute_reply.started":"2023-06-04T18:28:25.726750Z","shell.execute_reply":"2023-06-04T18:28:28.000745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python -m spacy download en_core_web_md ","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:28:28.003401Z","iopub.execute_input":"2023-06-04T18:28:28.003770Z","iopub.status.idle":"2023-06-04T18:28:28.008582Z","shell.execute_reply.started":"2023-06-04T18:28:28.003732Z","shell.execute_reply":"2023-06-04T18:28:28.007556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['NARATION'].fillna('', inplace=True)\ndata['NARATION'] = data['NARATION'].str.replace('[^A-Za-z0-9]+', ' ', regex=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:28:28.010498Z","iopub.execute_input":"2023-06-04T18:28:28.011342Z","iopub.status.idle":"2023-06-04T18:30:07.696967Z","shell.execute_reply.started":"2023-06-04T18:28:28.011303Z","shell.execute_reply":"2023-06-04T18:30:07.695854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nlp = spacy.load('en_core_web_sm')\n\n# def rem_names(text):\n#     # Create Doc object\n#     doc = nlp(text)\n    \n#     # Indentify the persons\n#     tokens = [ent.text for ent in doc.ents if ent.label_ != 'PERSON']\n#     text = ' '.join(tokens)\n#     print(text)\n#     return text\n\n# data['NARATION'] = data['NARATION'].apply(rem_names)\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:30:07.698448Z","iopub.execute_input":"2023-06-04T18:30:07.699394Z","iopub.status.idle":"2023-06-04T18:30:07.704679Z","shell.execute_reply.started":"2023-06-04T18:30:07.699361Z","shell.execute_reply":"2023-06-04T18:30:07.703481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Function to remove names from a text using NLTK\n# def remove_names(text):\n#     tokens = word_tokenize(text)\n#     nltk_results = ne_chunk(pos_tag(tokens))\n#     for nltk_result in nltk_results:\n#         if type(nltk_result) == Tree:\n#             for nltk_result_leaf in nltk_result.leaves():\n#                 tokens.remove(nltk_result_leaf[0])\n#     filtered_text = ' '.join(tokens)\n# #     print(filtered_text)\n#     return filtered_text\n\n# # Apply the name removal function to the \"NARATION\" column\n# data['NARATION'] = data['NARATION'].apply(remove_names)\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:30:07.705845Z","iopub.execute_input":"2023-06-04T18:30:07.706202Z","iopub.status.idle":"2023-06-04T18:30:07.720600Z","shell.execute_reply.started":"2023-06-04T18:30:07.706173Z","shell.execute_reply":"2023-06-04T18:30:07.718892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tried to remove human names but takes too long and after converting the characters from uppercase to lowercase these methods don't work i.e., cannot remove the names properly. If this is applied before converting the uppercase letters to lowercase the words in a sentence which are not human names get also removed even if it is not desired.","metadata":{}},{"cell_type":"code","source":"data['NARATION'] = data['NARATION'].str.lower()\ndata['DR_CR'] = data['DR_CR'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:30:07.722077Z","iopub.execute_input":"2023-06-04T18:30:07.722439Z","iopub.status.idle":"2023-06-04T18:30:28.361064Z","shell.execute_reply.started":"2023-06-04T18:30:07.722408Z","shell.execute_reply":"2023-06-04T18:30:28.359813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:30:28.365566Z","iopub.execute_input":"2023-06-04T18:30:28.365988Z","iopub.status.idle":"2023-06-04T18:30:28.381949Z","shell.execute_reply.started":"2023-06-04T18:30:28.365951Z","shell.execute_reply":"2023-06-04T18:30:28.380864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n# Perform tokenization, stemming, and stop word removal\nstop_words = set(stopwords.words('english'))\nstop_words |= {'cash', 'a', 'c', 'no', 'mm'}","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:30:28.383219Z","iopub.execute_input":"2023-06-04T18:30:28.383755Z","iopub.status.idle":"2023-06-04T18:30:28.397312Z","shell.execute_reply.started":"2023-06-04T18:30:28.383726Z","shell.execute_reply":"2023-06-04T18:30:28.396168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:30:28.399546Z","iopub.execute_input":"2023-06-04T18:30:28.400203Z","iopub.status.idle":"2023-06-04T18:30:30.112414Z","shell.execute_reply.started":"2023-06-04T18:30:28.400172Z","shell.execute_reply":"2023-06-04T18:30:30.111030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Define a function to perform lemmatization on a text\ndef lemmatize_text(text):\n    # Tokenize the text into words\n    tokens = word_tokenize(text)\n    # Lemmatize each word and join them back into a sentence\n    lemmatized_text = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    return lemmatized_text\n\n# Apply lemmatization to the \"NARATION\" column\ndata['Tokens'] = data['NARATION'].apply(lemmatize_text)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T18:30:30.114574Z","iopub.execute_input":"2023-06-04T18:30:30.115413Z","iopub.status.idle":"2023-06-04T19:34:40.615919Z","shell.execute_reply.started":"2023-06-04T18:30:30.115357Z","shell.execute_reply":"2023-06-04T19:34:40.614863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# withdrawal to withdraw and deposit/name to deposit\ntemp = []\n\nfor w in data['Tokens']:\n  c = []\n  for x in w:\n    if 'withdraw' in x:\n      x = 'withdraw'\n    if 'deposit' in x:\n      x = 'deposit'\n      # print(x)\n    c.append(x)\n  \n  temp.append(c)\n\nnarrations = temp\nlen(temp)\n# narrations","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = set(nltk.corpus.words.words())\nignored_words = ['eftn', 'ft', 'bkash', 'nogod', 'rtgs', 'pos', \n                 'cib', 'paywell', 'challan', 'npsb', 'dps', 'atm', 'trf', 'sonod']    # will be given in the chat\ntemp = []\n\nfor w in narrations:\n  c = []\n  for x in w:\n    if x in words or not x.isalpha() or x in ignored_words:\n     c.append(x)\n  \n  temp.append(c)\n  \nlen(temp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom spacy import displacy \nnlp = spacy.load('en_core_web_sm')\n\nNER_lists = set()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NER_lists = {'month', 'bari', 'china', 'german', 'zero', 'first', 'today', 'daily', 'mim', \n             'nova', 'bas', 'year', 'week', 'martin', 'two', 'june', 'monthly', 'khan', 'twelve', \n             'eighteen', 'quarterly', 'bakula', 'weekly', 'hour', 'august', 'annual', 'patwari', 'gore',\n             'fourteen', 'al', 'second', 'yesterday', 'shanghai', 'kokan', 'noon', 'fifteen', 'japan',\n             'sec', 'abu'}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"added_lists = ['ae', 'ad', 'mo', 'dada', 'hanif', 'brother', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', \n               'october', 'november','december', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'ka', 'mu', \n               'c', 'k', 'st', 'ar', 'dola', 'kaka', 'sima', 'point', 'service', 'palli', 'pally', 'rocky', 'dal', 'p', 'u', 'e', 'i', 'm', 'da', 'r', 'pu', \n               'hasan', 'begum', 'nandi', 'mullah', 'ge', 'amt', 'b', 'ba', 'bu', 'th', 'das', 'southeast', 'amin', 'rana', 'kazi', 'shahin', 'sir', 'amenia',\n               'sheik', 'saddik', 'dor', 'sri', 'i', 'id', 'son', 'cotton', 'road', 'boro', 'babu', 'na', 'x', 'name', 'date', 'title', 'para', 'new', 'l', 'rani', 'raj',\n               'sultana', 'chad', 'nid', 'barman', 'amir', 'doll', 'say', 'omer', 'mir', 'bibi', 'type', 'doc', 'jowel', 'tala', 'total', 'tony', 'bin', 'shah', 'sha',\n               'gate', 'orient', 'currier', 'coxs', 'razor', 'link', 'ghat', 'pur', 'mother', 'tara', 'tania', 'daud', 'outlet', 'fakir', 'sweety', 'tapu', 'dey', 'momo',\n               'puja', 'auto', 'ink', 'maria']\n\nfor x in added_lists:\n  if x not in ignored_words:\n     NER_lists.add(x)\n\nprint(NER_lists)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = []\n\nfor w in temp:\n  c = []\n  for x in w:\n    if x not in NER_lists or x in ignored_words:\n     c.append(x)\n  \n  final.append(c)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove empty narrations\n\ntemp1 = []\ntemp2 = []\nfor index, narration in enumerate(final):\n  if len(narration) != 0:\n    temp1.append(original_narrations.iloc[index])\n    temp2.append(narration)\n  # else:\n  #   print(original_narrations.iloc[index])\n\nprint(len(temp1))\nprint(len(temp2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize most frequent words\n\nimport itertools\n\nlemmatized_tokens = list(narrations)\ntoken_list = list(itertools.chain(*lemmatized_tokens))\ncounts_no = Counter(token_list)\nclean_texts = pd.DataFrame(counts_no.most_common (30), columns=['words', 'count'])\nfig, ax = plt.subplots(figsize=(8, 8))\nclean_texts.sort_values(by='count').plot.barh(x='words',y='count',ax=ax,color='blue')\nax.set_title(\"Most Frequently used words in Narrations\")\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flatten the list of tokens\nall_tokens = [token for sublist in data['Tokens'] for token in sublist]\n\n# Count the frequency of each word\nword_freq = Counter(all_tokens)\n\n# Convert the word frequencies to a DataFrame\ndf_freq = pd.DataFrame.from_dict(word_freq, orient='index', columns=['Frequency'])\ndf_freq.index.name = 'Word'\n\n# Sort the DataFrame by frequency in descending order\ndf_freq = df_freq.sort_values('Frequency', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:34:40.617452Z","iopub.execute_input":"2023-06-04T19:34:40.617840Z","iopub.status.idle":"2023-06-04T19:34:57.149845Z","shell.execute_reply.started":"2023-06-04T19:34:40.617796Z","shell.execute_reply":"2023-06-04T19:34:57.148664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Top 50 words bar plot\ndf_freq_top = df_freq[:50]\n# Plot the bar chart\nplt.figure(figsize=(10, 6))\ndf_freq_top.plot(kind='bar')\nplt.title('Word Frequency')\nplt.xlabel('Word')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:34:57.151654Z","iopub.execute_input":"2023-06-04T19:34:57.152444Z","iopub.status.idle":"2023-06-04T19:35:00.885789Z","shell.execute_reply.started":"2023-06-04T19:34:57.152398Z","shell.execute_reply":"2023-06-04T19:35:00.884884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntokens = data['Tokens'].apply(lambda x: ' '.join(x))  # Convert tokenized words back to string\n\n# Create a TF-IDF vectorizer object\nvectorizer = TfidfVectorizer()\n\n# Fit the vectorizer on the tokenized text\ntfidf_matrix = vectorizer.fit_transform(tokens)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:35:00.887251Z","iopub.execute_input":"2023-06-04T19:35:00.887930Z","iopub.status.idle":"2023-06-04T19:38:20.928849Z","shell.execute_reply.started":"2023-06-04T19:35:00.887888Z","shell.execute_reply":"2023-06-04T19:38:20.926170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the feature words (vocabulary) and their corresponding TF-IDF scores\nfeature_words = vectorizer.vocabulary_\nprint(feature_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Assuming you have a TF-IDF matrix called 'tfidf_matrix' or any other suitable data format\nk = 9  # Number of clusters\nkmeans = KMeans(n_clusters=k)\nkmeans.fit(tfidf_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:38:20.980452Z","iopub.execute_input":"2023-06-04T19:38:20.980879Z","iopub.status.idle":"2023-06-04T19:45:50.497882Z","shell.execute_reply.started":"2023-06-04T19:38:20.980848Z","shell.execute_reply":"2023-06-04T19:45:50.496864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_labels = kmeans.labels_\n# Assuming you have a DataFrame called 'df' with the original data\ndata['Cluster'] = cluster_labels  # Add a 'cluster' column to the DataFrame\n\n# Print the distribution of documents across clusters\nprint(data['Cluster'].value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:45:50.499633Z","iopub.execute_input":"2023-06-04T19:45:50.500474Z","iopub.status.idle":"2023-06-04T19:45:50.689795Z","shell.execute_reply.started":"2023-06-04T19:45:50.500439Z","shell.execute_reply":"2023-06-04T19:45:50.688619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nfrom scipy.sparse import csr_matrix\n\n# Reduce the dimensionality of the data to 2 dimensions using TruncatedSVD\nsvd = TruncatedSVD(n_components=2)\nreduced_features = svd.fit_transform(tfidf_matrix)\n\n# Get the cluster assignments\ncluster_labels = kmeans.labels_\n\n# Plot the cluster points\nplt.figure(figsize=(8, 6))\nfor i in range(k):\n    cluster_points = reduced_features[cluster_labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}')\n\nplt.title('Cluster Visualization')\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:45:50.691140Z","iopub.execute_input":"2023-06-04T19:45:50.692576Z","iopub.status.idle":"2023-06-04T19:50:57.136944Z","shell.execute_reply.started":"2023-06-04T19:45:50.692543Z","shell.execute_reply":"2023-06-04T19:50:57.135879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(20)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:54:40.013834Z","iopub.execute_input":"2023-06-04T19:54:40.014239Z","iopub.status.idle":"2023-06-04T19:54:40.042903Z","shell.execute_reply.started":"2023-06-04T19:54:40.014210Z","shell.execute_reply":"2023-06-04T19:54:40.041433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# # Assuming you have a list of tokenized sentences named 'tokens_list'\n\n# # Convert the tokenized sentences into strings\n# sentences = [' '.join(tokens) for tokens in data['Tokens']]\n\n# # Initialize the TfidfVectorizer\n# vectorizer = TfidfVectorizer()\n\n# # Fit and transform the sentences to obtain the TF-IDF matrix\n# tfidf_matrix = vectorizer.fit_transform(sentences)\n\n# # Get the feature words (vocabulary) from the vectorizer\n# feature_words = vectorizer.vocabulary_\n\n# # Print the feature words\n# print(feature_words)\n# print(data.head)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:50:57.138919Z","iopub.execute_input":"2023-06-04T19:50:57.139385Z","iopub.status.idle":"2023-06-04T19:50:57.145932Z","shell.execute_reply.started":"2023-06-04T19:50:57.139337Z","shell.execute_reply":"2023-06-04T19:50:57.144305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature_words_per_row = [list(set(row_tokens) & set(feature_words)) for row_tokens in data['Tokens']]\n\n# # data = data.assign(FeatureWords=[list(set(row_tokens) & set(feature_words)) for row_tokens in data['Tokens']])\n# # print(data.head())\n\n# from sklearn.cluster import KMeans\n\n# # Define the number of clusters\n# num_clusters = 5\n# amounts_array = data['AMOUNT'].to_numpy()\n\n# # Create a feature matrix by combining feature words and amounts\n# data_matrix = np.column_stack((feature_words_per_row, amounts_array))\n\n# # Perform K-means clustering\n# kmeans = KMeans(n_clusters=num_clusters)\n# kmeans.fit(data_matrix)\n\n# # Get the cluster labels\n# cluster_labels = kmeans.labels_\n\n# # Assign the cluster labels to your original dataframe\n# data['Cluster'] = cluster_labels\n# print(data.head())","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:50:57.147521Z","iopub.execute_input":"2023-06-04T19:50:57.147998Z","iopub.status.idle":"2023-06-04T19:50:57.167159Z","shell.execute_reply.started":"2023-06-04T19:50:57.147956Z","shell.execute_reply":"2023-06-04T19:50:57.166215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.cluster import KMeans\n\n# # Define the number of clusters\n# num_clusters = 5\n# amounts_array = data['AMOUNT'].to_numpy()\n\n# # Create a feature matrix by combining feature words and amounts\n# data_matrix = np.column_stack((feature_words_per_row, amounts_array))\n\n# # Perform K-means clustering\n# kmeans = KMeans(n_clusters=num_clusters)\n# kmeans.fit(data_matrix)\n\n# # Get the cluster labels\n# cluster_labels = kmeans.labels_\n\n# # Assign the cluster labels to your original dataframe\n# data['Cluster'] = cluster_labels","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:50:57.168631Z","iopub.execute_input":"2023-06-04T19:50:57.169756Z","iopub.status.idle":"2023-06-04T19:50:57.188399Z","shell.execute_reply.started":"2023-06-04T19:50:57.169714Z","shell.execute_reply":"2023-06-04T19:50:57.186832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.cluster import MiniBatchKMeans\n# from sklearn.feature_extraction.text import CountVectorizer\n\n\n# vectorizer = CountVectorizer()\n\n# # Fit and transform the documents to obtain the document-term matrix\n# X = vectorizer.fit_transform(data['NARATION'])\n\n# # Initialize the MiniBatchKMeans algorithm\n# kmeans = MiniBatchKMeans(n_clusters=3, random_state=42)\n\n# # Fit the algorithm on the document-term matrix\n# kmeans.fit(X)\n\n# # Get the cluster labels for each document\n# labels = kmeans.labels_\n \n# vec = TfidfVectorizer(stop_words=\"english\")\n# vec.fit(data.NARATION.values)\n# keywords = vec.transform(data.NARATION.values)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:50:57.190244Z","iopub.execute_input":"2023-06-04T19:50:57.191006Z","iopub.status.idle":"2023-06-04T19:50:57.202030Z","shell.execute_reply.started":"2023-06-04T19:50:57.190963Z","shell.execute_reply":"2023-06-04T19:50:57.200492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_data2 = data.groupby('AC_ID')\n\n# # Create a new DataFrame with a list of tuples for each distinct 'ac_no'\n# grouped_df = pd.DataFrame([(AC_ID, list(zip(group['AMOUNT'], group['DR_CR'], group['TOKENS'])))\n#                            for AC_ID, group in grouped_data2],\n#                           columns=['AC_ID', 'TRANSACTIONS'])\n# grouped_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:50:57.204091Z","iopub.execute_input":"2023-06-04T19:50:57.204555Z","iopub.status.idle":"2023-06-04T19:50:57.214064Z","shell.execute_reply.started":"2023-06-04T19:50:57.204502Z","shell.execute_reply":"2023-06-04T19:50:57.213071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# # Assuming 'grouped_df' is the DataFrame containing the 'transactions' column with a list of tuples\n# # Assuming the column name is 'transactions'\n# # def flat(d) -> str:\n# #     l = []\n# #     for tuple_item in d:\n# #         s = \"\"\n# #         for i in tuple_item:\n# #             s = s + \" \" + str(i)\n# #         l.append(s)\n# #     return l\n                        \n\n# # Flatten the 'transactions' column to create a list of strings\n# grouped_df['TRANSACTIONS'] = [[str(' '.join(str(element) for element in tuple_element)) for tuple_element in row] for row in grouped_df['TRANSACTIONS']]","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:50:57.215313Z","iopub.execute_input":"2023-06-04T19:50:57.215766Z","iopub.status.idle":"2023-06-04T19:50:57.231584Z","shell.execute_reply.started":"2023-06-04T19:50:57.215728Z","shell.execute_reply":"2023-06-04T19:50:57.230702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_df.head()\n# Initialize the TfidfVectorizer\n# vectorizer = TfidfVectorizer()\n\n# # Compute the TF-IDF features\n# tfidf_features = vectorizer.fit_transform(flatten_transactions)\n# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# # Assuming you have a dataframe named 'df' with a column 'text' containing the list of strings\n# # For example:\n# df = pd.DataFrame({'text': [['apple', 'banana', 'orange'], ['grape', 'kiwi']]})\n# print(df)\n\n# # Flatten the list of strings within each row\n# flattened_text = [word for sublist in df['text'] for word in sublist]\n# print(flattened_text)\n\n# # Initialize the TfidfVectorizer\n# vectorizer = TfidfVectorizer()\n\n# # Fit and transform the flattened text data to obtain the TF-IDF matrix\n# tfidf_matrix = vectorizer.fit_transform(flattened_text)\n\n\n# # Print the feature words\n# print(tfidf_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:50:57.233157Z","iopub.execute_input":"2023-06-04T19:50:57.233876Z","iopub.status.idle":"2023-06-04T19:50:57.244459Z","shell.execute_reply.started":"2023-06-04T19:50:57.233834Z","shell.execute_reply":"2023-06-04T19:50:57.243599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Convert the TF-IDF features to a DataFrame\n# tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=vectorizer.get_feature_names())\n\n# # Print the TF-IDF DataFrame\n# tfidf_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:50:57.252050Z","iopub.execute_input":"2023-06-04T19:50:57.252422Z","iopub.status.idle":"2023-06-04T19:50:57.260033Z","shell.execute_reply.started":"2023-06-04T19:50:57.252393Z","shell.execute_reply":"2023-06-04T19:50:57.258886Z"},"trusted":true},"execution_count":null,"outputs":[]}]}